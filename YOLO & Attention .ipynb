{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.0 ðŸš€ Python-3.8.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5938MiB)\n",
      "Setup complete âœ… (12 CPUs, 15.4 GB RAM, 329.8/456.0 GB disk)\n"
     ]
    }
   ],
   "source": [
    "from dataset import*\n",
    "from utility import*\n",
    "from baseline import*\n",
    "from transformer import*\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  access to a GPU\n",
      "0\n",
      "<torch.cuda.device object at 0x7f16743e4bb0>\n",
      "1\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {'' if torch.cuda.is_available() else 'not'} access to a GPU\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.current_device())\n",
    "    print(torch.cuda.device(0))\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = '/home/anto/University/Driving-Visual-Attention/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose size of the eyes\n",
    "dim = (32,64)\n",
    "# mean and std of images, calculated in advance\n",
    "mean = (0.4570, 0.4422, 0.3900)\n",
    "std = (0.2376, 0.2295, 0.2261)\n",
    "\n",
    "my_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(dim, antialias=True),\n",
    "    transforms.Normalize(mean=mean, std=mean, inplace=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset len is 22783\n"
     ]
    }
   ],
   "source": [
    "save_test_file = '/home/anto/University/Driving-Visual-Attention/save/save_test100'\n",
    "test_dataset = DGAZEDataset('test',save_test_file, my_transforms)\n",
    "print(f'Test dataset len is {len(test_dataset)}')\n",
    "test_dataloader = DataLoader(test_dataset,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = project_folder + '/EAI_Napoli/datasetCoco/data.yaml'\n",
    "#!yolo task=detect mode=train model=yolov8m.pt data=$dataset_path epochs=40 imgsz=640 pretrained=True batch=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model_path = project_folder + '/YOLO runs/runs/detect/train7/weights/best.pt'\n",
    "yolo_model = YOLO(yolo_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/anto/University/Driving-Visual-Attention/data/images_aligned/driver12/road_view/sample104/frame_0009.jpg: 384x640 4 persons, 1 car, 1 bus_stop, 108.6ms\n",
      "Speed: 1.8ms preprocess, 108.6ms inference, 358.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'bbox': array([1005,  114, 1920,  989]), 'class_name': 'bus_stop'}\n",
      "{'bbox': array([227, 578, 263, 694]), 'class_name': 'person'}\n",
      "{'bbox': array([851, 633, 971, 701]), 'class_name': 'car'}\n",
      "{'bbox': array([1307,  342, 1416,  452]), 'class_name': 'person'}\n",
      "{'bbox': array([ 73, 565, 116, 655]), 'class_name': 'person'}\n",
      "{'bbox': array([589, 597, 653, 718]), 'class_name': 'person'}\n"
     ]
    }
   ],
   "source": [
    "image1 = project_folder +'data/images_aligned/driver12/road_view/sample104/frame_0009.jpg'\n",
    "results = yolo_model(image1)  # return a list of Results objects\n",
    "result = results[0]\n",
    "boxes = result.boxes.cpu().numpy()  # Boxes object for bbox outputs\n",
    "for box in boxes:\n",
    "    # Extract bounding box coordinates as integers\n",
    "    bbox = box.xyxy[0].astype(int)\n",
    "    # Extract the classification name using the class index\n",
    "    class_index = int(box.cls[0])\n",
    "    class_name = result.names[class_index] \n",
    "    # Create a dictionary for the current bounding box and name\n",
    "    current_dict = {'bbox': bbox, 'class_name': class_name}\n",
    "    print(current_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Gaze Estimation+YOLO for Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GazeCNN(\n",
       "  (eye_feature_extractor): EyeFeatureExtractor(\n",
       "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu): LeakyReLU(negative_slope=0.01)\n",
       "    (block): ConvolutionBlock(\n",
       "      (conv_block): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm_block): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu_block): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (mlp_head): MLPHead(\n",
       "    (fc_additional): Sequential(\n",
       "      (0): Linear(in_features=7, out_features=16, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (fc_merged): Sequential(\n",
       "      (0): Linear(in_features=688, out_features=128, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=128, out_features=2, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaze_model = GazeCNN(additional_features_size=7)\n",
    "checkpoint_path = project_folder + '/save/baseline_epochs3_250.pth'\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "# Load the model state dictionary\n",
    "gaze_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "gaze_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 10505/22783 [06:10<07:06, 28.79it/s]"
     ]
    }
   ],
   "source": [
    "all_results = []  # To store the results for all images\n",
    "total_hits = 0  # To count the total number of times the point was inside any bounding box\n",
    "\n",
    "for eye,features,_, true_bbox, driver_path in tqdm(test_dataloader):\n",
    "    # Estimate the gaze \n",
    "    estimated_gaze = gaze_model(eye,features).squeeze(0).tolist()\n",
    "    estimated_gaze = tuple(estimated_gaze)\n",
    "    # Check if the gaze point is inside any true bounding box\n",
    "    true_bbox = tuple(true_bbox.squeeze(0).tolist())\n",
    "    is_inside_true_bbox = is_point_inside_bbox(estimated_gaze, true_bbox)\n",
    "    \n",
    "    # Run YOLO\n",
    "    road_path =  driver_path[0].replace('driver_view', 'road_view')\n",
    "    results = yolo_model(road_path, verbose=False)\n",
    "    result = results[0] # we pass only one image at a time\n",
    "    yolo_boxes = result.boxes.cpu().numpy()\n",
    "    names = result.names\n",
    "    for box in yolo_boxes:\n",
    "        # Extract bounding box coordinates as integers\n",
    "        bbox = box.xyxy[0].astype(int)\n",
    "        # Extract the classification name using the class index\n",
    "        class_index = int(box.cls[0])\n",
    "        class_name = result.names[class_index] \n",
    "        # Create a dictionary for the current bounding box and name\n",
    "        current_dict = {'bbox': bbox, 'class_name': class_name}\n",
    "        # Check if point is inside the bbox\n",
    "        is_inside_yolo_bbox = is_point_inside_bbox(estimated_gaze,bbox)\n",
    "        if is_inside_yolo_bbox and is_inside_true_bbox:\n",
    "            attention_score = 2\n",
    "            current_dict = {\n",
    "                'image_path': driver_path,\n",
    "                'attention_score': attention_score,\n",
    "                'obj_name': class_name\n",
    "            }\n",
    "            break\n",
    "        elif is_inside_yolo_bbox and not is_inside_true_bbox:\n",
    "            attention_score = 1\n",
    "            current_dict = {\n",
    "                'image_path': driver_path,\n",
    "                'attention_score': attention_score,\n",
    "                'obj_name': class_name\n",
    "            }\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    if not is_inside_true_bbox and not is_inside_yolo_bbox:\n",
    "        attention_score = 0\n",
    "\n",
    "    # Create a dictionary for the current image\n",
    "    current_dict = {\n",
    "        'image_path': driver_path,\n",
    "        'attention_score': attention_score,\n",
    "    }\n",
    "\n",
    "    # Append the dictionary to the list of results\n",
    "    all_results.append(current_dict)\n",
    "\n",
    "    # Update total_hits based on attention_score\n",
    "    if attention_score > 0:\n",
    "        total_hits += 1\n",
    "\n",
    "# Print the total number of hits\n",
    "print(f\"Total hits: {total_hits}\")\n",
    "\n",
    "# Print the results for all images\n",
    "print(\"Results:\")\n",
    "for result in all_results:\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
