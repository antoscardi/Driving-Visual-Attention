# ğŸš—ğŸ‘€ Visual Attention Estimation in Drivers ğŸš—ğŸ‘€
This is the repository for the project of the first module of the course **Electives in AI** held by **Prof. Christian Napoli** at **La Sapienza University of Rome**.
This project is based on this [paper](http://cdn.iiit.ac.in/cdn/cvit.iiit.ac.in/images/Projects/DGAZE/paper.pdf), which has developed the following dataset and model contained in this [repository](https://github.com/duaisha/DGAZE).  
 

## ğŸ” Overview
**Drivers' attention is key to road safety!** ğŸ›‘ğŸ‘€ This project aims to estimate where a driver is looking and whether they are paying attention to critical elements on the road. We use **Gaze Point Detection** and **Object Detection** to analyze visual focus and determine attentiveness.

## ğŸ—ï¸ Project Structure
ğŸ“‚ **`/models`** â€“ Pretrained and fine-tuned models (GazeCNN, Transformer, YOLOv8) ğŸ§ ğŸ’¡  
ğŸ“‚ **`/data`** â€“ DGaze dataset preprocessing and annotations ğŸ“ŠğŸ“Œ  
ğŸ“‚ **`/notebooks`** â€“ Jupyter notebooks for experiments ğŸ“–âœï¸  
ğŸ“‚ **`/scripts`** â€“ Training & evaluation scripts âš™ï¸ğŸ”¬  
ğŸ“‚ **`/results`** â€“ Model evaluation & analysis ğŸ“ˆğŸ¯  
ğŸ“‚ **`/docs`** â€“ Project documentation ğŸ“œğŸ–Šï¸  

## ğŸ¯ Key Features
âœ… **Gaze Estimation** using **CNN + Transformer** & **ResNet-based models** ğŸğŸš€  
âœ… **Object Detection** using **YOLOv8** to identify key road elements ğŸš˜ğŸš¦ğŸš¶  
âœ… **Attention Scoring** to classify attentiveness ğŸï¸ğŸ“  
âœ… **DGaze Dataset Analysis** (3761 image pairs) ğŸ“ŠğŸ“·  
âœ… **Evaluation against state-of-the-art** methods ğŸ†ğŸ“Œ  

## ğŸï¸ How It Works
1ï¸âƒ£ **Gaze Point Detection** ğŸ”­ğŸ‘€  
   - Predicts where the driver is looking using deep learning ğŸ§ ğŸ“  
2ï¸âƒ£ **Object Detection** ğŸš—ğŸ›‘  
   - YOLOv8 identifies key objects on the road ğŸğŸš¦  
3ï¸âƒ£ **Attention Analysis** ğŸ¯  
   - Matches the gaze point with detected objects âœ…ğŸ‘€  
   - Classifies attentiveness score (0: Not Attentive âŒ, 1: Distracted â—, 2: Focused âœ…)  

## ğŸ”¥ Model Performance
ğŸ“Œ **Best Model:** **CNN + Transformer (GazeTR-Hybrid)** ğŸ†ğŸ”¬  
ğŸ“Œ **Bounding Box Accuracy:** ~46% ğŸ¯ğŸ“¦  
ğŸ“Œ **Mean Absolute Error:** Competitive with state-of-the-art ğŸ¤–ğŸ“ˆ  
ğŸ“Œ **YOLO Performance:** 83.61% Precision, 73.99% Recall ğŸï¸ğŸš¦  

## ğŸš€ Installation & Usage
```bash
# Clone the repository ğŸ–¥ï¸
git clone https://github.com/your-username/VisualAttentionDrivers.git
cd VisualAttentionDrivers

# Install dependencies âš™ï¸
pip install -r requirements.txt

# Run inference (example script) ğŸ
python run_model.py --input_path data/sample_image.jpg
```

## ğŸ“Œ Results
ğŸ§ **Where do drivers focus the most?**  
- ğŸš— **Vehicles (cars & trucks) - 32.1%**  
- ğŸš¶ **Pedestrians - 8.3%**  
- ğŸ **Road Signs - 2.9%**  

## ğŸ¯ Future Work
âœ… Improve gaze estimation precision ğŸ¯ğŸ”¬  
âœ… Train on larger datasets for better generalization ğŸ“ŠğŸŒ  
âœ… Integrate real-time processing for ADAS ğŸš—âš¡  

## ğŸ‘¥ Contributors
-  [NiccolÃ² Piraino](https://github.com/Nickes10)
-  [Antonio Scardino](https://github.com/antoscardi)
 
