{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from dataset import*\n","from utility import*\n","from training import*\n","from baseline import*\n","from transformer import*\n","\n","# Head pose DL model from https://github.com/thohemp/6drepnet\n","from sixdrepnet import SixDRepNet\n","import dlib\n","\n","from torch.optim import lr_scheduler \n","# Import models\n","from torchvision import models\n","from vit_pytorch.twins_svt import TwinsSVT # MEMORIA NON SUFFICIENTE RIPROVARLO\n","#from vit_pytorch.vit import ViT\n","from vit_pytorch.ats_vit import ViT\n","from vit_pytorch import SimpleViT\n","from vit_pytorch.crossformer import CrossFormer # MEMORIA NON SUFFICIENTE RIPROVARLO\n","from vit_pytorch.cross_vit import CrossViT"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["root_project = '/home/anto/University/Driving-Visual-Attention/'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We have  access to a GPU\n","0\n","<torch.cuda.device object at 0x7f80241be310>\n","1\n","NVIDIA GeForce RTX 3060 Laptop GPU\n","cuda\n"]}],"source":["print(f\"We have {'' if torch.cuda.is_available() else 'not'} access to a GPU\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(torch.cuda.current_device())\n","    print(torch.cuda.device(0))\n","    print(torch.cuda.device_count())\n","    print(torch.cuda.get_device_name(0))\n","print(device)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["##### Initialize pre-trained models for feature extraction"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Initialize face detector and facial landmarks predictor\n","predictor = dlib.shape_predictor(\"/home/anto/University/Driving-Visual-Attention/data/shape_predictor_68_face_landmarks.dat\")\n","headpose_extractor = SixDRepNet()\n","face_detector = dlib.get_frontal_face_detector()"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loader and Visualization"]},{"cell_type":"markdown","metadata":{},"source":["##### Files where to write the paths and labels"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["percentage = 100\n","save_train_file = root_project + 'save/save_train' + str(percentage)#+'_complete'\n","save_val_file = root_project + 'save/save_val' + str(percentage)#+'_complete'\n","save_test_file = root_project + 'save/save_test' + str(percentage)#+'_complete'"]},{"cell_type":"markdown","metadata":{},"source":["##### Train Validation and Test Loader"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["#train_dataset_classloader = DataLoaderVisualizer(root_project, save_train_file, percentage, predictor, face_detector, headpose_extractor, 'train',big_file=False)\n","#val_dataset_classloader = DataLoaderVisualizer(root_project,save_val_file,percentage,predictor, face_detector, headpose_extractor,'val',big_file=False)\n","#test_dataset_classloader = DataLoaderVisualizer(root_project,save_val_file,percentage,predictor, face_detector, headpose_extractor,'test',big_file=False)"]},{"cell_type":"markdown","metadata":{},"source":["##### Visualization"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["#train_dataset_classloader.visualize_dataset()\n","#val_dataset_classloader.visualize_dataset()\n","#test_dataset_classloader.visualize_dataset()"]},{"cell_type":"markdown","metadata":{},"source":["### Pytorch Dataset "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Choose size of the eyes\n","dim = (32,64)\n","# mean and std of images, calculated in advance\n","mean = (0.4570, 0.4422, 0.3900)\n","std = (0.2376, 0.2295, 0.2261)\n","\n","my_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize(dim, antialias=True),\n","    transforms.Normalize(mean=mean, std=mean, inplace=True)\n","])"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset len is 187458\n"]}],"source":["train_dataset = DGAZEDataset('train', save_train_file, my_transforms, big_file=False)\n","print(f'Train dataset len is {len(train_dataset)}')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"]},{"name":"stdout","output_type":"stream","text":["The bbox is: tensor([1397.,  506., 2550., 1010.])\n","The additional features are: tensor([ -7.3503,  26.1545,  -2.1894, 227.0000, 487.0000, 367.0000, 488.0000])\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Print an example of the dataset for correct visualization\n","img_np = train_dataset[10000][0].permute(1, 2, 0).numpy()*255\n","print(f\"The bbox is: {train_dataset[30][3]}\")\n","print(f\"The additional features are: {train_dataset[30][1]}\")\n","plt.imshow(img_np)\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Val dataset len is 23317\n"]}],"source":["val_dataset = DGAZEDataset('val',save_val_file,my_transforms, big_file=False)\n","print(f'Val dataset len is {len(val_dataset)}')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["#test_dataset = DGAZEDataset('test',save_test_file, my_transforms)\n","#print(f'Test dataset len is {len(test_dataset)}')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Unite datasets, increse samples in the training or validation\n","#from torch.utils.data import ConcatDataset\n","#val_dataset = ConcatDataset([val_dataset,test_dataset])\n","#train_dataset = ConcatDataset([train_dataset,test_dataset])"]},{"cell_type":"markdown","metadata":{},"source":["### Vision Transformer Model"]},{"cell_type":"markdown","metadata":{},"source":["##### Hyerparameters"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["EPOCHS = 10\n","BATCH_SIZE = 16\n","THRESHOLD = 250\n","LR = 0.001\n","BETAS = (0.9, 0.97)\n","WEIGHT_DECAY = 1e-5\n","STEP_SIZE = 15000\n","GAMMA = 0.1\n","pre_trained = False"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["GazeCNN(\n","  (eye_feature_extractor): EyeFeatureExtractor(\n","    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu): LeakyReLU(negative_slope=0.01)\n","    (block): ConvolutionBlock(\n","      (conv_block): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (batch_norm_block): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu_block): LeakyReLU(negative_slope=0.01)\n","    )\n","    (pool): MaxPool2d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  )\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (mlp_head): MLPHead(\n","    (fc_additional): Sequential(\n","      (0): Linear(in_features=7, out_features=16, bias=True)\n","      (1): LeakyReLU(negative_slope=0.01)\n","    )\n","    (fc_merged): Sequential(\n","      (0): Linear(in_features=688, out_features=64, bias=True)\n","      (1): LeakyReLU(negative_slope=0.01)\n","      (2): Linear(in_features=64, out_features=2, bias=True)\n","      (3): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n",")"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["model = GazeCNN(additional_features_size=7)\n","#model = CNNTrans()\n","#tensor2 = torch.randn(64,3, 64, 128)\n","#tensor1 = torch.randn(64,7)\n","#out = model(tensor2,tensor1)\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["##### Criterion and Optimizer"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["bbox_accuracy_class = BBoxAccuracy()\n","criterion = nn.L1Loss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=BETAS, weight_decay= WEIGHT_DECAY)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"]},{"cell_type":"markdown","metadata":{},"source":["##### Dataloader"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Training "]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["if pre_trained:\n","    ckpt_path = ''\n","    checkpoint = torch.load(ckpt_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"6QZlMOAH0znJ"},"outputs":[{"ename":"AttributeError","evalue":"module 'wandb' has no attribute 'login'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogin\u001b[49m()\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNNbaseline best\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTHRESHOLD\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, normalization,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercentage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=percent, weight_decay=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWEIGHT_DECAY\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,betas=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBETAS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, gamma=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGAMMA\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, step_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSTEP_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mAttributeError\u001b[0m: module 'wandb' has no attribute 'login'"]}],"source":["wandb.login()\n","wandb.init(project=\"CNNbaseline best\", name=f\"threshold={THRESHOLD}, batch_size={BATCH_SIZE}, normalization,{percentage}=percent, weight_decay={WEIGHT_DECAY}, lr={LR},betas={BETAS}, gamma={GAMMA}, step_size={STEP_SIZE}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if pre_trained:\n","    start_epoch = checkpoint['epoch']\n","    EPOCHS = start_epoch + EPOCHS\n","else:\n","    start_epoch = 0\n","    EPOCHS = EPOCHS\n","\n","for epoch in range(start_epoch, EPOCHS):\n","    # Training\n","    train_loss = train_epoch(model, train_loader, criterion, scheduler, optimizer, device, epoch)\n","    wandb.log({\"epoch\": epoch + 1,\"train_loss\": train_loss})\n","\n","    # Validation\n","    val_loss, val_accuracy, bbox_accuracy, error = validate(model, bbox_accuracy_class , val_loader, THRESHOLD, criterion, device, epoch, BATCH_SIZE)\n","    wandb.log({\"epoch\": epoch + 1,\"val_loss\": val_loss})\n","    wandb.log({\"epoch\": epoch + 1,\"accuracy_threshold\": val_accuracy*100})\n","    wandb.log({\"epoch\": epoch + 1,\"accuracy_bbox\": bbox_accuracy*100})\n","    wandb.log({\"epoch\": epoch + 1,\"error\": error})\n","\n","    #log_image(val_loader, model, device)\n","\n","# Finish the WandB run\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_dict = {\n","    'epoch': epoch,\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","}\n","torch.save(save_dict, root_project + 'save/test' + str(EPOCHS)+'_'+str(THRESHOLD)+'.pth')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMTeTAy7+Meohhx8z6EH3FB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
