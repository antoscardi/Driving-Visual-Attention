{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Requirements"]},{"cell_type":"code","execution_count":78,"metadata":{},"outputs":[],"source":["from dataset import*\n","from utility import*\n","from training import*\n","from baseline import*\n","from transformer import*\n","\n","# Head pose DL model from https://github.com/thohemp/6drepnet\n","from sixdrepnet import SixDRepNet\n","import dlib\n","\n","from torch.optim import lr_scheduler \n","# Import models\n","from torchvision import models\n","from vit_pytorch.twins_svt import TwinsSVT # MEMORIA NON SUFFICIENTE RIPROVARLO\n","#from vit_pytorch.vit import ViT\n","from vit_pytorch.ats_vit import ViT\n","from vit_pytorch import SimpleViT\n","from vit_pytorch.crossformer import CrossFormer # MEMORIA NON SUFFICIENTE RIPROVARLO\n","from vit_pytorch.cross_vit import CrossViT"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["root_project = '/home/anto/University/Driving-Visual-Attention/'"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We have  access to a GPU\n","0\n","<torch.cuda.device object at 0x7f63a141bac0>\n","1\n","NVIDIA GeForce RTX 3060 Laptop GPU\n","cuda\n"]}],"source":["print(f\"We have {'' if torch.cuda.is_available() else 'not'} access to a GPU\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(torch.cuda.current_device())\n","    print(torch.cuda.device(0))\n","    print(torch.cuda.device_count())\n","    print(torch.cuda.get_device_name(0))\n","print(device)"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["##### Initialize pre-trained models for feature extraction"]},{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["# Initialize face detector and facial landmarks predictor\n","predictor = dlib.shape_predictor(\"/home/anto/University/Driving-Visual-Attention/data/shape_predictor_68_face_landmarks.dat\")\n","headpose_extractor = SixDRepNet()\n","face_detector = dlib.get_frontal_face_detector()"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loader and Visualization"]},{"cell_type":"markdown","metadata":{},"source":["##### Files where to write the paths and labels"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["percentage = 100\n","save_train_file = root_project + 'save/save_train' + str(percentage)\n","save_val_file = root_project + 'save/save_val' + str(percentage)\n","save_test_file = root_project + 'save/save_test' + str(percentage)"]},{"cell_type":"markdown","metadata":{},"source":["##### Train Loader"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","The dataset has already been prepared, ready to use\n"]}],"source":["train_dataset_classloader = DataLoaderVisualizer(root_project, save_train_file, percentage, predictor, face_detector, headpose_extractor, 'train')"]},{"cell_type":"markdown","metadata":{},"source":["##### Validtion Loader"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","The dataset has already been prepared, ready to use\n"]}],"source":["val_dataset_classloader = DataLoaderVisualizer(root_project,save_val_file,percentage,predictor, face_detector, headpose_extractor,'val')"]},{"cell_type":"markdown","metadata":{},"source":["##### Test Loader"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","The dataset has already been prepared, ready to use\n"]}],"source":["test_dataset_classloader = DataLoaderVisualizer(root_project,save_test_file,percentage,predictor, face_detector, headpose_extractor,'test')"]},{"cell_type":"markdown","metadata":{},"source":["##### Visualization"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["#train_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["#val_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["#test_dataset_classloader.visualize_dataset()"]},{"cell_type":"markdown","metadata":{},"source":["### Pytorch Dataset "]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["# Choose size of the eyes\n","dim = (32,64)\n","# mean and std of images, calculated in advance\n","mean = (0.4570, 0.4422, 0.3900)\n","std = (0.2376, 0.2295, 0.2261)\n","\n","my_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize(dim, antialias=True),\n","    transforms.Normalize(mean=mean, std=mean, inplace=True)\n","])"]},{"cell_type":"code","execution_count":91,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset len is 187458\n"]}],"source":["train_dataset = DGAZEDataset('train','save/save_train'+str(percentage), my_transforms)\n","print(f'Train dataset len is {len(train_dataset)}')"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"]},{"name":"stdout","output_type":"stream","text":["The bbox is: tensor([1397.,  506., 2550., 1010.])\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# Print an example of the dataset for correct visualization\n","img_np = train_dataset[30][0].permute(1, 2, 0).numpy()*255\n","print(f\"The bbox is: {train_dataset[30][3]}\")\n","plt.imshow(img_np)\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":93,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Val dataset len is 23317\n"]}],"source":["val_dataset = DGAZEDataset('val','save/save_val'+str(percentage),my_transforms)\n","print(f'Val dataset len is {len(val_dataset)}')"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test dataset len is 22783\n"]}],"source":["test_dataset = DGAZEDataset('test','save/save_test'+str(percentage),my_transforms)\n","print(f'Test dataset len is {len(test_dataset)}')"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["# Unite datasets, increse samples in the training or validation\n","#from torch.utils.data import ConcatDataset\n","#val_dataset = ConcatDataset([val_dataset,test_dataset])\n","#train_dataset = ConcatDataset([train_dataset,test_dataset])"]},{"cell_type":"markdown","metadata":{},"source":["### Vision Transformer Model"]},{"cell_type":"markdown","metadata":{},"source":["##### Hyerparameters"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[],"source":["EPOCHS = 5\n","BATCH_SIZE = 16\n","THRESHOLD = 250\n","LR = 0.001\n","BETAS = (0.9, 0.97)\n","WEIGHT_DECAY = 1e-5\n","STEP_SIZE = 15000\n","GAMMA = 0.1\n","pre_trained = False"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"data":{"text/plain":["GazeCNN(\n","  (eye_feature_extractor): EyeFeatureExtractor(\n","    (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (relu): LeakyReLU(negative_slope=0.01)\n","    (block): ConvolutionBlock(\n","      (conv_block): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (batch_norm_block): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu_block): LeakyReLU(negative_slope=0.01)\n","    )\n","    (pool): MaxPool2d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (dropout): Dropout(p=0.1, inplace=False)\n","    (conv2): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  )\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (mlp_head): MLPHead(\n","    (fc_additional): Sequential(\n","      (0): Linear(in_features=7, out_features=16, bias=True)\n","      (1): LeakyReLU(negative_slope=0.01)\n","    )\n","    (fc_merged): Sequential(\n","      (0): Linear(in_features=352, out_features=64, bias=True)\n","      (1): LeakyReLU(negative_slope=0.01)\n","      (2): Linear(in_features=64, out_features=2, bias=True)\n","      (3): LeakyReLU(negative_slope=0.01)\n","    )\n","  )\n",")"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["model = GazeCNN()\n","#model = CNNTrans()\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["##### Criterion and Optimizer"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[],"source":["bbox_accuracy_class = BBoxAccuracy()\n","criterion = nn.L1Loss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=LR, betas=BETAS, weight_decay= WEIGHT_DECAY)\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)"]},{"cell_type":"markdown","metadata":{},"source":["##### Dataloader"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Training "]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[],"source":["if pre_trained:\n","    ckpt_path = ''\n","    checkpoint = torch.load(ckpt_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":101,"metadata":{"id":"6QZlMOAH0znJ"},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"]},{"data":{"text/html":["Finishing last run (ID:nvi75l6i) before initializing another..."],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"]},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">threshold=250, batch_size=16, normalization,100=percent, weight_decay=1e-05, lr=0.001,betas=(0.9, 0.97), gamma=0.1, step_size=15000</strong> at: <a href='https://wandb.ai/vesuvio-erutta/Best%20Baseline%20CNN/runs/nvi75l6i' target=\"_blank\">https://wandb.ai/vesuvio-erutta/Best%20Baseline%20CNN/runs/nvi75l6i</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240113_013314-nvi75l6i/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Successfully finished last run (ID:nvi75l6i). Initializing new run:<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.2"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/anto/University/Driving-Visual-Attention/wandb/run-20240113_013400-4nj2jxik</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/vesuvio-erutta/Best%20Baseline%20CNN/runs/4nj2jxik' target=\"_blank\">threshold=250, batch_size=16, normalization,100=percent, weight_decay=1e-05, lr=0.001,betas=(0.9, 0.97), gamma=0.1, step_size=15000</a></strong> to <a href='https://wandb.ai/vesuvio-erutta/Best%20Baseline%20CNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/vesuvio-erutta/Best%20Baseline%20CNN' target=\"_blank\">https://wandb.ai/vesuvio-erutta/Best%20Baseline%20CNN</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/vesuvio-erutta/Best%20Baseline%20CNN/runs/4nj2jxik' target=\"_blank\">https://wandb.ai/vesuvio-erutta/Best%20Baseline%20CNN/runs/4nj2jxik</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/vesuvio-erutta/Best%20Baseline%20CNN/runs/4nj2jxik?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f63e402a310>"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()\n","wandb.init(project=\"Best Baseline CNN\", name=f\"threshold={THRESHOLD}, batch_size={BATCH_SIZE}, normalization,{percentage}=percent, weight_decay={WEIGHT_DECAY}, lr={LR},betas={BETAS}, gamma={GAMMA}, step_size={STEP_SIZE}\")"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 0: 100%|██████████| 11716/11716 [01:47<00:00, 108.68batch/s, batch loss=145.21]\n","Validation Epoch 0: 100%|██████████| 1458/1458 [00:15<00:00, 93.51batch/s, batch accuracy=25.00%] \n","Training Epoch 1: 100%|██████████| 11716/11716 [01:55<00:00, 101.18batch/s, batch loss=246.94]\n","Validation Epoch 1: 100%|██████████| 1458/1458 [00:15<00:00, 93.58batch/s, batch accuracy=25.00%] \n","Training Epoch 2: 100%|██████████| 11716/11716 [01:55<00:00, 101.65batch/s, batch loss=178.56]\n","Validation Epoch 2: 100%|██████████| 1458/1458 [00:15<00:00, 95.31batch/s, batch accuracy=25.00%] \n","Training Epoch 3: 100%|██████████| 11716/11716 [01:47<00:00, 109.27batch/s, batch loss=233.49]\n","Validation Epoch 3: 100%|██████████| 1458/1458 [00:12<00:00, 112.85batch/s, batch accuracy=25.00%] \n","Training Epoch 4: 100%|██████████| 11716/11716 [01:40<00:00, 116.01batch/s, batch loss=170.80]\n","Validation Epoch 4: 100%|██████████| 1458/1458 [00:12<00:00, 116.67batch/s, batch accuracy=25.00%] \n","wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"]},{"data":{"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy_bbox</td><td>▁████</td></tr><tr><td>accuracy_paper(error)</td><td>█▂▁▁▁</td></tr><tr><td>accuracy_threshold</td><td>▁▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▃▃▃▃▃▅▅▅▅▅▆▆▆▆▆█████</td></tr><tr><td>train_loss</td><td>█▂▁▁▁</td></tr><tr><td>val_loss</td><td>█▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy_bbox</td><td>23.96262</td></tr><tr><td>accuracy_paper(error)</td><td>281.18744</td></tr><tr><td>accuracy_threshold</td><td>54.39386</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>train_loss</td><td>176.45266</td></tr><tr><td>val_loss</td><td>175.50335</td></tr></table><br/></div></div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run <strong style=\"color:#cdcd00\">threshold=250, batch_size=16, normalization,100=percent, weight_decay=1e-05, lr=0.001,betas=(0.9, 0.97), gamma=0.1, step_size=15000</strong> at: <a href='https://wandb.ai/vesuvio-erutta/Best%20Baseline%20CNN/runs/4nj2jxik' target=\"_blank\">https://wandb.ai/vesuvio-erutta/Best%20Baseline%20CNN/runs/4nj2jxik</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20240113_013400-4nj2jxik/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["if pre_trained:\n","    start_epoch = checkpoint['epoch']\n","    EPOCHS = start_epoch + EPOCHS\n","else:\n","    start_epoch = 0\n","    EPOCHS = EPOCHS\n","\n","for epoch in range(start_epoch, EPOCHS):\n","    # Training\n","    train_loss = train_epoch(model, train_loader, criterion, scheduler, optimizer, device, epoch)\n","    wandb.log({\"epoch\": epoch + 1,\"train_loss\": train_loss})\n","\n","    # Validation\n","    val_loss, val_accuracy, bbox_accuracy, paper_accuracy = validate(model, bbox_accuracy_class , val_loader, THRESHOLD, criterion, device, epoch, BATCH_SIZE)\n","    wandb.log({\"epoch\": epoch + 1,\"val_loss\": val_loss})\n","    wandb.log({\"epoch\": epoch + 1,\"accuracy_threshold\": val_accuracy*100})\n","    wandb.log({\"epoch\": epoch + 1,\"accuracy_bbox\": bbox_accuracy*100})\n","    wandb.log({\"epoch\": epoch + 1,\"accuracy_paper(error)\": paper_accuracy})\n","\n","    #log_image(val_loader, model, device)\n","\n","# Finish the WandB run\n","wandb.finish()"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["save_dict = {\n","    'epoch': epoch,\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","}\n","torch.save(save_dict, root_project + 'save/best_CNN_baseline'+'.pth')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMTeTAy7+Meohhx8z6EH3FB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
