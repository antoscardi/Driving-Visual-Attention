{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.0 ðŸš€ Python-3.8.18 torch-2.1.2+cu121 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5938MiB)\n",
      "Setup complete âœ… (12 CPUs, 15.4 GB RAM, 330.8/456.0 GB disk)\n"
     ]
    }
   ],
   "source": [
    "from dataset import*\n",
    "from utility import*\n",
    "from baseline import*\n",
    "from transformer import*\n",
    "from training import *\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  access to a GPU\n",
      "0\n",
      "<torch.cuda.device object at 0x7f253a76d490>\n",
      "1\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"We have {'' if torch.cuda.is_available() else 'not'} access to a GPU\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.current_device())\n",
    "    print(torch.cuda.device(0))\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = '/home/anto/University/Driving-Visual-Attention/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose size of the eyes\n",
    "dim = (32,64)\n",
    "# mean and std of images, calculated in advance\n",
    "mean = (0.4570, 0.4422, 0.3900)\n",
    "std = (0.2376, 0.2295, 0.2261)\n",
    "\n",
    "my_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(dim, antialias=True),\n",
    "    transforms.Normalize(mean=mean, std=mean, inplace=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1\n",
    "BATCH_SIZE = 1\n",
    "THRESHOLD = 250\n",
    "bbox_accuracy_class = BBoxAccuracy()\n",
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset len is 22783\n"
     ]
    }
   ],
   "source": [
    "save_test_file = '/home/anto/University/Driving-Visual-Attention/save/save_test100'\n",
    "test_dataset = DGAZEDataset('test',save_test_file, my_transforms, big_file=False)\n",
    "print(f'Test dataset len is {len(test_dataset)}')\n",
    "test_dataloader = DataLoader(test_dataset,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = project_folder + '/EAI_Napoli/datasetCoco/data.yaml'\n",
    "#!yolo task=detect mode=train model=yolov8m.pt data=$dataset_path epochs=40 imgsz=640 pretrained=True batch=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model_path = project_folder + '/save/yolo_best.pt'\n",
    "yolo_model = YOLO(yolo_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/anto/University/Driving-Visual-Attention/data/images_aligned/driver12/road_view/sample104/frame_0009.jpg: 384x640 2 persons, 2 cars, 1 bus_stop, 111.9ms\n",
      "Speed: 2.0ms preprocess, 111.9ms inference, 330.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "{'bbox': array([ 999,  111, 1920, 1012]), 'class_name': 'bus_stop'}\n",
      "{'bbox': array([221, 578, 264, 690]), 'class_name': 'person'}\n",
      "{'bbox': array([586, 628, 666, 717]), 'class_name': 'person'}\n",
      "{'bbox': array([738, 645, 843, 706]), 'class_name': 'car'}\n",
      "{'bbox': array([865, 627, 968, 701]), 'class_name': 'car'}\n"
     ]
    }
   ],
   "source": [
    "image1 = project_folder +'data/images_aligned/driver12/road_view/sample104/frame_0009.jpg'\n",
    "results = yolo_model(image1)  # return a list of Results objects\n",
    "result = results[0]\n",
    "boxes = result.boxes.cpu().numpy()  # Boxes object for bbox outputs\n",
    "for box in boxes:\n",
    "    # Extract bounding box coordinates as integers\n",
    "    bbox = box.xyxy[0].astype(int)\n",
    "    # Extract the classification name using the class index\n",
    "    class_index = int(box.cls[0])\n",
    "    class_name = result.names[class_index] \n",
    "    # Create a dictionary for the current bounding box and name\n",
    "    current_dict = {'bbox': bbox, 'class_name': class_name}\n",
    "    print(current_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Gaze Estimation+YOLO for Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GazeCNN(\n",
       "  (eye_feature_extractor): EyeFeatureExtractor(\n",
       "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu): LeakyReLU(negative_slope=0.01)\n",
       "    (block): ConvolutionBlock(\n",
       "      (conv_block): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (batch_norm_block): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu_block): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=4, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (conv2): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (mlp_head): MLPHead(\n",
       "    (fc_additional): Sequential(\n",
       "      (0): Linear(in_features=7, out_features=16, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (fc_merged): Sequential(\n",
       "      (0): Linear(in_features=688, out_features=64, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=64, out_features=2, bias=True)\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaze_model = GazeCNN(additional_features_size=7)\n",
    "checkpoint_path = project_folder + '/save/best_CNN_baseline_64acc.pth'\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "# Load the model state dictionary\n",
    "gaze_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "gaze_model.eval()\n",
    "gaze_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:09<00:00,  4.71batch/s, batch accuracy=20.20%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss 228.46461254617444\n",
      "accuracy_threshold 37.56521924034409\n",
      "accuracy_bbox 15.969565154417701\n",
      "accuracy_paper(error) 371.93436232857084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # Testing\n",
    "    test_loss, test_accuracy, bbox_accuracy, error = validate(gaze_model, bbox_accuracy_class , test_dataloader, THRESHOLD, criterion, device, epoch, BATCH_SIZE)\n",
    "    #log_image(val_loader, model, device)\n",
    "\n",
    "print(\"test_loss\", test_loss)\n",
    "print(\"accuracy_threshold\", test_accuracy*100)\n",
    "print(\"accuracy_bbox\", bbox_accuracy*100)\n",
    "print(\"error\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_Plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/46 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Check if the gaze point is inside any true bounding box\u001b[39;00m\n\u001b[1;32m      8\u001b[0m true_bbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(true_bbox\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[0;32m----> 9\u001b[0m is_inside_true_bbox \u001b[38;5;241m=\u001b[39m \u001b[43mis_point_inside_bbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimated_gaze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_bbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Run YOLO\u001b[39;00m\n\u001b[1;32m     12\u001b[0m road_path \u001b[38;5;241m=\u001b[39m  driver_path[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdriver_view\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroad_view\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/University/Driving-Visual-Attention/include/utility.py:150\u001b[0m, in \u001b[0;36mis_point_inside_bbox\u001b[0;34m(point, bbox)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_point_inside_bbox\u001b[39m(point, bbox):\n\u001b[0;32m--> 150\u001b[0m         x, y \u001b[38;5;241m=\u001b[39m point\n\u001b[1;32m    151\u001b[0m         x1, y1, x2, y2 \u001b[38;5;241m=\u001b[39m bbox                                                              \n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(x1, x2) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(x1, x2) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(y1, y2) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m y \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(y1, y2)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "all_results = []  # To store the results for all images\n",
    "for eye,features,_, true_bbox, driver_path in tqdm(test_dataloader):\n",
    "    eye, features = eye.to(device), features.to(device)\n",
    "    # Estimate the gaze \n",
    "    estimated_gaze = gaze_model(eye,features).squeeze(0).tolist()\n",
    "    estimated_gaze = tuple(estimated_gaze)\n",
    "    # Check if the gaze point is inside any true bounding box\n",
    "    true_bbox = tuple(true_bbox.squeeze(0).tolist())\n",
    "    is_inside_true_bbox = is_point_inside_bbox(estimated_gaze, true_bbox)\n",
    "    \n",
    "    # Run YOLO\n",
    "    road_path =  driver_path[0].replace('driver_view', 'road_view')\n",
    "    results = yolo_model(road_path, verbose=False)\n",
    "    result = results[0] # we pass only one image at a time\n",
    "    yolo_boxes = result.boxes.cpu().numpy()\n",
    "    names = result.names\n",
    "    for box in yolo_boxes:\n",
    "        # Extract bounding box coordinates as integers\n",
    "        bbox = box.xyxy[0].astype(int)\n",
    "        # Extract the classification name using the class index\n",
    "        class_index = int(box.cls[0])\n",
    "        class_name = result.names[class_index] \n",
    "        # Check if point is inside the bbox\n",
    "        is_inside_yolo_bbox = is_point_inside_bbox(estimated_gaze,bbox)\n",
    "        if is_inside_yolo_bbox and is_inside_true_bbox:\n",
    "            attention_score = 2\n",
    "            current_dict = {\n",
    "                'image_path': driver_path,\n",
    "                'attention_score': attention_score,\n",
    "                'obj_name': class_name\n",
    "            }\n",
    "            break\n",
    "        elif is_inside_yolo_bbox and not is_inside_true_bbox:\n",
    "            attention_score = 1\n",
    "            current_dict = {\n",
    "                'image_path': driver_path,\n",
    "                'attention_score': attention_score,\n",
    "                'obj_name': class_name\n",
    "            }\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    if not is_inside_true_bbox and not is_inside_yolo_bbox:\n",
    "        attention_score = 0\n",
    "        current_dict = {\n",
    "            'image_path': driver_path,\n",
    "            'attention_score': attention_score,\n",
    "        }\n",
    "\n",
    "    if do_Plot:\n",
    "        if 'obj_name' in current_dict:\n",
    "            ### PLOT\n",
    "            road_photo = cv2.imread(road_path)\n",
    "            road_photo = cv2.cvtColor(road_photo, cv2.COLOR_BGR2RGB)\n",
    "            plt.imshow(road_photo)\n",
    "            plt.axis('off')\n",
    "            fig, ax = plt.subplots(1)\n",
    "            gaze_x, gaze_y = estimated_gaze\n",
    "            ax.plot(gaze_x, gaze_y, 'ro', markersize=25)\n",
    "            for box in yolo_boxes:\n",
    "                bbox = box.xyxy[0].astype(int)\n",
    "                rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],\n",
    "                                        linewidth=2, edgecolor='lightcoral', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "            # Add text annotation for attention score and object name\n",
    "            object = current_dict['obj_name']\n",
    "            ax.text(10, 10, f'Attention Score: {attention_score}\\nObject: {object}',\n",
    "                    color='red', fontsize=10, bbox=dict(facecolor='black', alpha=0.7))\n",
    "            ax.imshow(road_photo)\n",
    "            ax.axis('off')\n",
    "            plt.show()\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Append the dictionary to the list of results\n",
    "    all_results.append(current_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Extract the class names from the 'obj_name' key (if it exists)\n",
    "class_names = [item.get('obj_name', 'Gaze outside any box') for item in all_results]\n",
    "\n",
    "# Count the occurrences of each class\n",
    "class_counts = Counter(class_names)\n",
    "\n",
    "# Display the result\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f'{class_name}: {count/len(all_results)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "wrong = 0\n",
    "semi = 0\n",
    "for dict in all_results:\n",
    "    attention_score = dict.get('attention_score')\n",
    "    if attention_score == 2:\n",
    "        correct += 1\n",
    "    if attention_score == 0:\n",
    "        wrong +=1\n",
    "    if attention_score == 1:\n",
    "        semi +=1\n",
    "print(f'Inside correct bbox: {correct/len(all_results)*100:.2f}%')\n",
    "print(f'Inside another bbox: {semi/len(all_results)*100:.2f}%')\n",
    "print(f'Inside NO bbox: {wrong/len(all_results)*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
