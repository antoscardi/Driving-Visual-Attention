{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import*\n",
    "from utility import*\n",
    "from baseline import*\n",
    "from transformer import*\n",
    "import ultralytics\n",
    "from ultralytics import YOLO\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {'' if torch.cuda.is_available() else 'not'} access to a GPU\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.current_device())\n",
    "    print(torch.cuda.device(0))\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = '/home/anto/University/Driving-Visual-Attention/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose size of the eyes\n",
    "dim = (32,64)\n",
    "# mean and std of images, calculated in advance\n",
    "mean = (0.4570, 0.4422, 0.3900)\n",
    "std = (0.2376, 0.2295, 0.2261)\n",
    "\n",
    "my_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(dim, antialias=True),\n",
    "    transforms.Normalize(mean=mean, std=mean, inplace=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_test_file = '/home/anto/University/Driving-Visual-Attention/save/save_test100'\n",
    "test_dataset = DGAZEDataset('test',save_test_file, my_transforms)\n",
    "print(f'Test dataset len is {len(test_dataset)}')\n",
    "test_dataloader = DataLoader(test_dataset,1,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_path = project_folder + '/EAI_Napoli/datasetCoco/data.yaml'\n",
    "#!yolo task=detect mode=train model=yolov8m.pt data=$dataset_path epochs=40 imgsz=640 pretrained=True batch=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_model_path = project_folder + '/YOLO runs/runs/detect/train7/weights/best.pt'\n",
    "yolo_model = YOLO(yolo_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1 = project_folder +'data/images_aligned/driver12/road_view/sample104/frame_0009.jpg'\n",
    "results = yolo_model(image1)  # return a list of Results objects\n",
    "result = results[0]\n",
    "boxes = result.boxes.cpu().numpy()  # Boxes object for bbox outputs\n",
    "for box in boxes:\n",
    "    # Extract bounding box coordinates as integers\n",
    "    bbox = box.xyxy[0].astype(int)\n",
    "    # Extract the classification name using the class index\n",
    "    class_index = int(box.cls[0])\n",
    "    class_name = result.names[class_index] \n",
    "    # Create a dictionary for the current bounding box and name\n",
    "    current_dict = {'bbox': bbox, 'class_name': class_name}\n",
    "    print(current_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Gaze Estimation+YOLO for Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaze_model = GazeCNN(additional_features_size=7)\n",
    "checkpoint_path = project_folder + '/save/best_CNN_baseline_64acc.pth'\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "# Load the model state dictionary\n",
    "gaze_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "gaze_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_Plot = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []  # To store the results for all images\n",
    "for eye,features,_, true_bbox, driver_path in tqdm(test_dataloader):\n",
    "    # Estimate the gaze \n",
    "    estimated_gaze = gaze_model(eye,features).squeeze(0).tolist()\n",
    "    estimated_gaze = tuple(estimated_gaze)\n",
    "    # Check if the gaze point is inside any true bounding box\n",
    "    true_bbox = tuple(true_bbox.squeeze(0).tolist())\n",
    "    is_inside_true_bbox = is_point_inside_bbox(estimated_gaze, true_bbox)\n",
    "    \n",
    "    # Run YOLO\n",
    "    road_path =  driver_path[0].replace('driver_view', 'road_view')\n",
    "    results = yolo_model(road_path, verbose=False)\n",
    "    result = results[0] # we pass only one image at a time\n",
    "    yolo_boxes = result.boxes.cpu().numpy()\n",
    "    names = result.names\n",
    "    for box in yolo_boxes:\n",
    "        # Extract bounding box coordinates as integers\n",
    "        bbox = box.xyxy[0].astype(int)\n",
    "        # Extract the classification name using the class index\n",
    "        class_index = int(box.cls[0])\n",
    "        class_name = result.names[class_index] \n",
    "        # Check if point is inside the bbox\n",
    "        is_inside_yolo_bbox = is_point_inside_bbox(estimated_gaze,bbox)\n",
    "        if is_inside_yolo_bbox and is_inside_true_bbox:\n",
    "            attention_score = 2\n",
    "            current_dict = {\n",
    "                'image_path': driver_path,\n",
    "                'attention_score': attention_score,\n",
    "                'obj_name': class_name\n",
    "            }\n",
    "            break\n",
    "        elif is_inside_yolo_bbox and not is_inside_true_bbox:\n",
    "            attention_score = 1\n",
    "            current_dict = {\n",
    "                'image_path': driver_path,\n",
    "                'attention_score': attention_score,\n",
    "                'obj_name': class_name\n",
    "            }\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    if not is_inside_true_bbox and not is_inside_yolo_bbox:\n",
    "        attention_score = 0\n",
    "        current_dict = {\n",
    "            'image_path': driver_path,\n",
    "            'attention_score': attention_score,\n",
    "        }\n",
    "\n",
    "    if do_Plot:\n",
    "        if 'obj_name' in current_dict:\n",
    "            ### PLOT\n",
    "            road_photo = cv2.imread(road_path)\n",
    "            road_photo = cv2.cvtColor(road_photo, cv2.COLOR_BGR2RGB)\n",
    "            plt.imshow(road_photo)\n",
    "            plt.axis('off')\n",
    "            fig, ax = plt.subplots(1)\n",
    "            gaze_x, gaze_y = estimated_gaze\n",
    "            ax.plot(gaze_x, gaze_y, 'ro', markersize=25)\n",
    "            for box in yolo_boxes:\n",
    "                bbox = box.xyxy[0].astype(int)\n",
    "                rect = patches.Rectangle((bbox[0], bbox[1]), bbox[2] - bbox[0], bbox[3] - bbox[1],\n",
    "                                        linewidth=2, edgecolor='lightcoral', facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "            # Add text annotation for attention score and object name\n",
    "            object = current_dict['obj_name']\n",
    "            ax.text(10, 10, f'Attention Score: {attention_score}\\nObject: {object}',\n",
    "                    color='red', fontsize=10, bbox=dict(facecolor='black', alpha=0.7))\n",
    "            ax.imshow(road_photo)\n",
    "            ax.axis('off')\n",
    "            plt.show()\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Append the dictionary to the list of results\n",
    "    all_results.append(current_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Extract the class names from the 'obj_name' key (if it exists)\n",
    "class_names = [item.get('obj_name', 'Gaze outside any box') for item in all_results]\n",
    "\n",
    "# Count the occurrences of each class\n",
    "class_counts = Counter(class_names)\n",
    "\n",
    "# Display the result\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f'{class_name}: {count/len(all_results)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "wrong = 0\n",
    "semi = 0\n",
    "for dict in all_results:\n",
    "    attention_score = dict.get('attention_score')\n",
    "    if attention_score == 2:\n",
    "        correct += 1\n",
    "    if attention_score == 0:\n",
    "        wrong +=1\n",
    "    if attention_score == 1:\n",
    "        semi +=1\n",
    "print(f'Inside correct bbox: {correct/len(all_results)*100:.2f}%')\n",
    "print(f'Inside another bbox: {semi/len(all_results)*100:.2f}%')\n",
    "print(f'Inside NO bbox: {wrong/len(all_results)*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
