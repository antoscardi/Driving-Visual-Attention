{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from dataset import*\n","from utility import*\n","from training import *\n","from models import *\n","from vit_pytorch.vit import ViT\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["root_project = '/home/anto/University/Driving-Visual-Attention/'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We have not access to a GPU\n","cpu\n"]},{"name":"stderr","output_type":"stream","text":["/home/anto/Apps/Anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n","  return torch._C._cuda_getDeviceCount() > 0\n"]}],"source":["print(f\"We have {'' if torch.cuda.is_available() else 'not'} access to a GPU\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(torch.cuda.current_device())\n","    print(torch.cuda.device(0))\n","    print(torch.cuda.device_count())\n","    print(torch.cuda.get_device_name(0))\n","print(device)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loader and Visualization"]},{"cell_type":"markdown","metadata":{},"source":["##### Files where to write the paths and labels"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["percentage = 1\n","save_train_file = root_project + 'save/save_train' + str(percentage)\n","save_val_file = root_project + 'save/save_val' + str(percentage)\n","save_test_file = root_project + 'save/save_test' + str(percentage)"]},{"cell_type":"markdown","metadata":{},"source":["##### Train Loader"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","The dataset has already been prepared, ready to use\n"]}],"source":["train_dataset_classloader = DataLoaderVisualizer(root_project,save_train_file,percentage,'train')"]},{"cell_type":"markdown","metadata":{},"source":["##### Validtion Loader"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","The dataset has already been prepared, ready to use\n"]}],"source":["val_dataset_classloader = DataLoaderVisualizer(root_project,save_val_file,percentage,'val')"]},{"cell_type":"markdown","metadata":{},"source":["##### Test Loader"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","The dataset has already been prepared, ready to use\n"]}],"source":["test_dataset_classloader = DataLoaderVisualizer(root_project,save_test_file,percentage,'test')"]},{"cell_type":"markdown","metadata":{},"source":["##### Visualization"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["#train_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#val_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#test_dataset_classloader.visualize_dataset()"]},{"cell_type":"markdown","metadata":{},"source":["### Pytorch Dataset "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Crop and convert to tensor\n","crop_params = (125, 75, 768, 768)\n","# mean and std of images, calculated in advance\n","mean = (0.4573337137699127, 0.4427291750907898, 0.3902426064014435)\n","std = (0.23664842545986176, 0.22875066101551056, 0.2255575954914093)\n","\n","my_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    CropTransform(crop_params),\n","    transforms.Normalize(mean=mean, std=mean, inplace=True)\n","])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset len is 1054\n"]}],"source":["train_dataset = DGAZEDataset('train','save/save_train1',my_transforms)\n","print(f'Train dataset len is {len(train_dataset)}')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["'tensor_img = train_dataset[100][0]*255\\nnumpy_img = tensor_img.permute(1, 2, 0).numpy().astype(np.uint8)\\nfrom PIL import Image\\nfrom IPython.display import display\\n# Convert to PIL Image\\nimage_pil = Image.fromarray(numpy_img)\\n\\n# Display the image in the notebook\\ndisplay(image_pil)'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["'''tensor_img = train_dataset[100][0]*255\n","numpy_img = tensor_img.permute(1, 2, 0).numpy().astype(np.uint8)\n","from PIL import Image\n","from IPython.display import display\n","# Convert to PIL Image\n","image_pil = Image.fromarray(numpy_img)\n","\n","# Display the image in the notebook\n","display(image_pil)'''"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Val dataset len is 127\n"]}],"source":["val_dataset = DGAZEDataset('val','save/save_val1',my_transforms)\n","print(f'Val dataset len is {len(val_dataset)}')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test dataset len is 126\n"]}],"source":["test_dataset = DGAZEDataset('test','save/save_test1',my_transforms)\n","print(f'Test dataset len is {len(test_dataset)}')"]},{"cell_type":"markdown","metadata":{},"source":["### Vision Transformer Model"]},{"cell_type":"markdown","metadata":{},"source":["##### Hyerparameters"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["BATCH_SIZE = 16\n","EPOCHS = 10"]},{"cell_type":"markdown","metadata":{},"source":["##### ResNet + Transformer model"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["ViT(\n","  (to_patch_embedding): Sequential(\n","    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=64, p2=64)\n","    (1): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)\n","    (2): Linear(in_features=12288, out_features=1024, bias=True)\n","    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (transformer): Transformer(\n","    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (layers): ModuleList(\n","      (0-3): 4 x ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (to_qkv): Linear(in_features=1024, out_features=1536, bias=False)\n","          (to_out): Sequential(\n","            (0): Linear(in_features=512, out_features=1024, bias=True)\n","            (1): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=1024, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Dropout(p=0.1, inplace=False)\n","            (4): Linear(in_features=1024, out_features=1024, bias=True)\n","            (5): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (to_latent): Identity()\n","  (mlp_head): Linear(in_features=1024, out_features=2, bias=True)\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["model = ReSViT()\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["##### Criterion and Optimizer"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["#criterion = nn.MSELoss()\n","criterion = nn.L1Loss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)"]},{"cell_type":"markdown","metadata":{},"source":["##### Dataloader"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Training "]},{"cell_type":"code","execution_count":21,"metadata":{"id":"6QZlMOAH0znJ"},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mscardino-2020613\u001b[0m (\u001b[33mvesuvio-erutta\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]}],"source":["import wandb\n","wandb.login()\n","# Percentage of validation data to log to wandb for visualization\n","random_percentage = 0.1  "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.16.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/anto/University/Driving-Visual-Attention/wandb/run-20231229_174441-o4zybfmo</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/vesuvio-erutta/GazeViT%20pulito/runs/o4zybfmo' target=\"_blank\">train 1 percent 10 epoche </a></strong> to <a href='https://wandb.ai/vesuvio-erutta/GazeViT%20pulito' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/vesuvio-erutta/GazeViT%20pulito' target=\"_blank\">https://wandb.ai/vesuvio-erutta/GazeViT%20pulito</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/vesuvio-erutta/GazeViT%20pulito/runs/o4zybfmo' target=\"_blank\">https://wandb.ai/vesuvio-erutta/GazeViT%20pulito/runs/o4zybfmo</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/vesuvio-erutta/GazeViT%20pulito/runs/o4zybfmo?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f7a1b6c0ed0>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(project=\"ResViT\", name=\"train 1 percent 10 epoche \")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 0: 100%|██████████| 65/65 [02:34<00:00,  2.37s/batch, batch loss=707.6353149414062]\n","validation Epoch 0: 100%|██████████| 8/8 [00:08<00:00,  1.01s/batch, batch accuracy=0.0%]         \n","Training Epoch 1:  89%|████████▉ | 58/65 [02:21<00:18,  2.62s/batch, batch loss=655.518798828125] "]}],"source":["for epoch in range(EPOCHS):\n","    # Training\n","    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n","    wandb.log({\"train_loss\": train_loss, \"epoch\": epoch + 1})\n","\n","    # Validation\n","    val_loss, val_accuracy = validate(model, val_loader, criterion, device, epoch)\n","    wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"epoch\": epoch + 1})\n","\n","    with torch.no_grad():\n","        model.eval()\n","\n","        # Log example images and predictions \n","        example_batch = next(iter(val_loader))\n","        example_images, example_labels, img_paths = example_batch\n","\n","        # Calculate gaze point prediction\n","        example_predictions = model(example_images.to(device))\n","\n","        # Convert tensors to numpy arrays for visualization\n","        example_images_np = example_images.detach().numpy()\n","        example_predictions_np = example_predictions.cpu().detach().numpy()\n","        example_labels_np = example_labels.detach().numpy()\n","\n","        # Log a random percentage of images with predictions\n","        random_indices = log_random_images(example_images_np, random_percentage)\n","\n","        images_with_predictions = []\n","\n","        for idx in random_indices:\n","            img, pred, label, img_path = (\n","                example_images_np[idx],\n","                example_predictions_np[idx],\n","                example_labels_np[idx],\n","                img_paths[idx],\n","            )\n","\n","            # Load the road_view image\n","            respective_road_view = img_path.replace('driver_view', 'road_view')\n","            \n","            # Mark the road-view with the prediction\n","            road_view_image = mark_image(respective_road_view, tuple(pred.astype(int)), tuple(label.astype(int)))\n","\n","            # Log the annotated road_view image to WandB\n","            images_with_predictions.append(\n","                wandb.Image(\n","                    road_view_image,\n","                    caption=f\"Prediction: {pred}, Actual: {label}\",\n","                )\n","            )\n","\n","        wandb.log({\"examples\": images_with_predictions, \"epoch\": epoch + 1})\n","\n","# Finish the WandB run\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["### Test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(model.state_dict(), root_project + 'save/model_weights_epochs' + str(EPOCHS)+ '.pth')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMTeTAy7+Meohhx8z6EH3FB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
