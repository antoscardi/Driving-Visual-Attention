{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from dataset import*\n","from utility import*\n","from training import *\n","from vit_pytorch.vit import ViT"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["root_project = '/home/anto/University/Driving-Visual-Attention/'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We have not access to a GPU\n","cpu\n"]},{"name":"stderr","output_type":"stream","text":["/home/anto/Apps/Anaconda3/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n","  return torch._C._cuda_getDeviceCount() > 0\n"]}],"source":["print(f\"We have {'' if torch.cuda.is_available() else 'not'} access to a GPU\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(torch.cuda.current_device())\n","    print(torch.cuda.device(0))\n","    print(torch.cuda.device_count())\n","    print(torch.cuda.get_device_name(0))\n","print(device)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loader and Visualization"]},{"cell_type":"markdown","metadata":{},"source":["##### Files where to write the paths and labels"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["percentage = 1\n","save_train_file = root_project + 'save/save_train' + str(percentage)\n","save_val_file = root_project + 'save/save_val' + str(percentage)\n","save_test_file = root_project + 'save/save_test' + str(percentage)"]},{"cell_type":"markdown","metadata":{},"source":["##### Train Loader"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","The dataset has already been prepared, ready to use\n"]}],"source":["train_dataset_classloader = DataLoaderVisualizer(root_project,save_train_file,percentage,'train')"]},{"cell_type":"markdown","metadata":{},"source":["##### Validtion Loader"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","The dataset has already been prepared, ready to use\n"]}],"source":["val_dataset_classloader = DataLoaderVisualizer(root_project,save_val_file,percentage,'val')"]},{"cell_type":"markdown","metadata":{},"source":["##### Test Loader"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","The dataset has already been prepared, ready to use\n"]}],"source":["test_dataset_classloader = DataLoaderVisualizer(root_project,save_test_file,percentage,'test')"]},{"cell_type":"markdown","metadata":{},"source":["##### Visualization"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["#train_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#val_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#test_dataset_classloader.visualize_dataset()"]},{"cell_type":"markdown","metadata":{},"source":["### Pytorch Dataset "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Crop and convert to tensor\n","crop_params = (125, 75, 768, 768)\n","\n","my_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    CropTransform(crop_params),\n","    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset len is 1054\n"]}],"source":["train_dataset = DGAZEDataset('train','save/save_train1',my_transforms)\n","print(f'Train dataset len is {len(train_dataset)}')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Val dataset len is 127\n"]}],"source":["val_dataset = DGAZEDataset('val','save/save_val1',my_transforms)\n","print(f'Val dataset len is {len(val_dataset)}')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test dataset len is 126\n"]}],"source":["test_dataset = DGAZEDataset('test','save/save_test1',my_transforms)\n","print(f'Test dataset len is {len(test_dataset)}')"]},{"cell_type":"markdown","metadata":{},"source":["### Vision Transformer Model"]},{"cell_type":"markdown","metadata":{},"source":["##### Hyerparameters"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["BATCH_SIZE = 32\n","EPOCHS = 15\n","IMAGE_SIZE = 768\n","HEADS = 6\n","DROPOUT = 0.1\n","PATCH_SIZE = 32\n","DIM = 1024\n","MLP_DIM= 1024\n","DEPTH = 3"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"data":{"text/plain":["ViT(\n","  (to_patch_embedding): Sequential(\n","    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=32, p2=32)\n","    (1): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n","    (2): Linear(in_features=3072, out_features=1024, bias=True)\n","    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (transformer): Transformer(\n","    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (layers): ModuleList(\n","      (0-2): 3 x ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (to_qkv): Linear(in_features=1024, out_features=1152, bias=False)\n","          (to_out): Sequential(\n","            (0): Linear(in_features=384, out_features=1024, bias=True)\n","            (1): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=1024, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Dropout(p=0.1, inplace=False)\n","            (4): Linear(in_features=1024, out_features=1024, bias=True)\n","            (5): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (to_latent): Identity()\n","  (mlp_head): Linear(in_features=1024, out_features=2, bias=True)\n",")"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["model = ViT(\n","    image_size = IMAGE_SIZE,\n","    patch_size = PATCH_SIZE,\n","    num_classes = 2,\n","    dim = DIM,\n","    depth = DEPTH,\n","    heads = HEADS,\n","    mlp_dim = MLP_DIM,\n","    dropout = DROPOUT,\n","    emb_dropout = 0.1\n",")\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["##### Criterion and Optimizer"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["#criterion = nn.MSELoss()\n","criterion = nn.L1Loss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"]},{"cell_type":"markdown","metadata":{},"source":["##### Dataloader"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4\n"]}],"source":["print(len(val_loader))"]},{"cell_type":"markdown","metadata":{},"source":["### Training "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QZlMOAH0znJ"},"outputs":[],"source":["import wandb\n","wandb.login()\n","# Percentage of validation data to log to wandb for visualization\n","random_percentage = 0.2  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wandb.init(project=\"GazeViT\", name=\"train 1 percent 15 epoche\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for epoch in range(EPOCHS):\n","    # Training\n","    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n","    wandb.log({\"train_loss\": train_loss, \"epoch\": epoch + 1})\n","\n","    # Validation\n","    val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n","    wandb.log({\"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"epoch\": epoch + 1})\n","\n","    with torch.no_grad():\n","        model.eval()\n","\n","        # Log example images and predictions \n","        example_batch = next(iter(val_loader))\n","        example_images, example_labels, img_paths = example_batch\n","\n","        # Calculate gaze point prediction\n","        example_predictions = model(example_images.to(device))\n","\n","        # Convert tensors to numpy arrays for visualization\n","        example_images_np = example_images.detach().numpy()\n","        example_predictions_np = example_predictions.cpu().detach().numpy()\n","        example_labels_np = example_labels.detach().numpy()\n","\n","        # Log a random percentage of images with predictions\n","        random_indices = log_random_images(example_images_np, random_percentage)\n","\n","        images_with_predictions = []\n","\n","        for idx in random_indices:\n","            img, pred, label, img_path = (\n","                example_images_np[idx],\n","                example_predictions_np[idx],\n","                example_labels_np[idx],\n","                img_paths[idx],\n","            )\n","\n","            # Load the road_view image\n","            respective_road_view = img_path.replace('driver_view', 'road_view')\n","            \n","            # Mark the road-view with the prediction\n","            road_view_image = mark_image(respective_road_view, tuple(pred.astype(int)), tuple(label.astype(int)))\n","\n","            # Log the annotated road_view image to WandB\n","            images_with_predictions.append(\n","                wandb.Image(\n","                    road_view_image,\n","                    caption=f\"Prediction: {pred}, Actual: {label}\",\n","                )\n","            )\n","\n","        wandb.log({\"examples\": images_with_predictions, \"epoch\": epoch + 1})\n","\n","# Finish the WandB run\n","wandb.finish()"]},{"cell_type":"markdown","metadata":{},"source":["### Test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["torch.save(model.state_dict(), root_project + 'save/model_weights.pth')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMTeTAy7+Meohhx8z6EH3FB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
