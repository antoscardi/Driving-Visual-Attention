{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from dataset import*\n","from utility import*\n","from training import *\n","from vit_pytorch.vit import ViT"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["root_project = '/home/anto/University/Driving-Visual-Attention/'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We have  access to a GPU\n","0\n","<torch.cuda.device object at 0x7f41e842cf10>\n","1\n","NVIDIA GeForce RTX 3060 Laptop GPU\n","cuda\n"]}],"source":["print(f\"We have {'' if torch.cuda.is_available() else 'not'} access to a GPU\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(torch.cuda.current_device())\n","    print(torch.cuda.device(0))\n","    print(torch.cuda.device_count())\n","    print(torch.cuda.get_device_name(0))\n","print(device)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loader and Visualization"]},{"cell_type":"markdown","metadata":{},"source":["##### Files where to write the paths and labels"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["percentage = 1\n","save_train_file = root_project + 'save/save_train' + str(percentage)\n","save_val_file = root_project + 'save/save_val' + str(percentage)\n","save_test_file = root_project + 'save/save_test' + str(percentage)"]},{"cell_type":"markdown","metadata":{},"source":["##### Train Loader"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","The dataset has already been prepared, ready to use\n"]}],"source":["train_dataset_classloader = DataLoaderVisualizer(root_project,save_train_file,percentage,'train')"]},{"cell_type":"markdown","metadata":{},"source":["##### Validtion Loader"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","The dataset has already been prepared, ready to use\n"]}],"source":["val_dataset_classloader = DataLoaderVisualizer(root_project,save_val_file,percentage,'val')"]},{"cell_type":"markdown","metadata":{},"source":["##### Test Loader"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","The dataset has already been prepared, ready to use\n"]}],"source":["test_dataset_classloader = DataLoaderVisualizer(root_project,save_test_file,percentage,'test')"]},{"cell_type":"markdown","metadata":{},"source":["##### Visualization"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["#train_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#val_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#test_dataset_classloader.visualize_dataset()"]},{"cell_type":"markdown","metadata":{},"source":["### Pytorch Dataset "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Crop and convert to tensor\n","crop_params = (125, 75, 768, 768)\n","# mean and std of images, calculated in advance\n","mean = (0.4573337137699127, 0.4427291750907898, 0.3902426064014435)\n","std = (0.23664842545986176, 0.22875066101551056, 0.2255575954914093)\n","\n","my_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    CropTransform(crop_params),\n","    transforms.Normalize(mean=mean, std=mean, inplace=True)\n","])"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset len is 1054\n"]}],"source":["train_dataset = DGAZEDataset('train','save/save_train1',my_transforms)\n","print(f'Train dataset len is {len(train_dataset)}')"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/plain":["'tensor_img = train_dataset[100][0]*255\\nnumpy_img = tensor_img.permute(1, 2, 0).numpy().astype(np.uint8)\\nfrom PIL import Image\\nfrom IPython.display import display\\n# Convert to PIL Image\\nimage_pil = Image.fromarray(numpy_img)\\n\\n# Display the image in the notebook\\ndisplay(image_pil)'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["'''tensor_img = train_dataset[100][0]*255\n","numpy_img = tensor_img.permute(1, 2, 0).numpy().astype(np.uint8)\n","from PIL import Image\n","from IPython.display import display\n","# Convert to PIL Image\n","image_pil = Image.fromarray(numpy_img)\n","\n","# Display the image in the notebook\n","display(image_pil)'''"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Val dataset len is 127\n"]}],"source":["val_dataset = DGAZEDataset('val','save/save_val1',my_transforms)\n","print(f'Val dataset len is {len(val_dataset)}')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test dataset len is 126\n"]}],"source":["test_dataset = DGAZEDataset('test','save/save_test1',my_transforms)\n","print(f'Test dataset len is {len(test_dataset)}')"]},{"cell_type":"markdown","metadata":{},"source":["### Vision Transformer Model"]},{"cell_type":"markdown","metadata":{},"source":["##### Hyerparameters"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["EPOCHS = 10\n","BATCH_SIZE = 16\n","IMAGE_SIZE = 768\n","HEADS = 8\n","DROPOUT = 0.1\n","PATCH_SIZE = 64\n","DIM = 1024\n","MLP_DIM= 1024\n","DEPTH = 4\n","pre_trained = False"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["ViT(\n","  (to_patch_embedding): Sequential(\n","    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=64, p2=64)\n","    (1): LayerNorm((12288,), eps=1e-05, elementwise_affine=True)\n","    (2): Linear(in_features=12288, out_features=1024, bias=True)\n","    (3): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (transformer): Transformer(\n","    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","    (layers): ModuleList(\n","      (0-3): 4 x ModuleList(\n","        (0): Attention(\n","          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","          (attend): Softmax(dim=-1)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (to_qkv): Linear(in_features=1024, out_features=1536, bias=False)\n","          (to_out): Sequential(\n","            (0): Linear(in_features=512, out_features=1024, bias=True)\n","            (1): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): FeedForward(\n","          (net): Sequential(\n","            (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","            (1): Linear(in_features=1024, out_features=1024, bias=True)\n","            (2): GELU(approximate='none')\n","            (3): Dropout(p=0.1, inplace=False)\n","            (4): Linear(in_features=1024, out_features=1024, bias=True)\n","            (5): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (to_latent): Identity()\n","  (mlp_head): Linear(in_features=1024, out_features=2, bias=True)\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["model = ViT(\n","    image_size = IMAGE_SIZE,\n","    patch_size = PATCH_SIZE,\n","    num_classes = 2,\n","    channels= 3,\n","    pool='mean',\n","    dim = DIM,\n","    depth = DEPTH,\n","    heads = HEADS,\n","    mlp_dim = MLP_DIM,\n","    dropout = DROPOUT,\n","    emb_dropout = 0.1\n",")\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["##### Criterion and Optimizer"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["#criterion = nn.MSELoss()\n","criterion = nn.L1Loss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01)"]},{"cell_type":"markdown","metadata":{},"source":["##### Dataloader"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Training "]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["if pre_trained:\n","    ckpt_path = '/home/anto/University/Driving-Visual-Attention/save/model_weights_epochs10.pth'\n","    checkpoint = torch.load(ckpt_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"6QZlMOAH0znJ"},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mscardino-2020613\u001b[0m (\u001b[33mvesuvio-erutta\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Tracking run with wandb version 0.16.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/anto/University/Driving-Visual-Attention/wandb/run-20231229_205603-1hejc4cr</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/vesuvio-erutta/GazeViT/runs/1hejc4cr' target=\"_blank\">10 epoche lr=0.01</a></strong> to <a href='https://wandb.ai/vesuvio-erutta/GazeViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/vesuvio-erutta/GazeViT' target=\"_blank\">https://wandb.ai/vesuvio-erutta/GazeViT</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/vesuvio-erutta/GazeViT/runs/1hejc4cr' target=\"_blank\">https://wandb.ai/vesuvio-erutta/GazeViT/runs/1hejc4cr</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/vesuvio-erutta/GazeViT/runs/1hejc4cr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7f41c7b2f690>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()\n","wandb.init(project=\"GazeViT\", name=\"10 epoche lr=0.01\")"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 0:  34%|███▍      | 22/65 [00:10<00:21,  2.04batch/s, batch loss=549.52]"]}],"source":["if pre_trained:\n","    start_epoch = checkpoint['epoch']\n","    EPOCHS = start_epoch + EPOCHS\n","else:\n","    start_epoch = 0\n","    EPOCHS = EPOCHS\n","\n","for epoch in range(start_epoch, EPOCHS):\n","    # Training\n","    train_loss = train_epoch(model, train_loader, criterion, optimizer, device, epoch)\n","    wandb.log({\"epoch\": epoch + 1,\"train_loss\": train_loss})\n","\n","    # Validation\n","    val_loss, val_accuracy = validate(model, val_loader, criterion, device, epoch)\n","    wandb.log({\"epoch\": epoch + 1,\"val_loss\": val_loss})\n","    wandb.log({\"epoch\": epoch + 1,\"val_accuracy\": val_accuracy})\n","\n","    log_image(val_loader, model, device)\n","\n","    # Finish the WandB run\n","    wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_dict = {\n","    'epoch': epoch,\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","}\n","torch.save(save_dict, root_project + 'save/GazeVit_weights_epochs' + str(EPOCHS)+ '.pth')"]},{"cell_type":"markdown","metadata":{},"source":["### Test"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMTeTAy7+Meohhx8z6EH3FB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}
