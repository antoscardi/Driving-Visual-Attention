{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from dataset import*\n","from utility import*\n","from training import*\n","from models import*\n","from torch.optim import lr_scheduler \n","\n","# Head pose DL model from https://github.com/thohemp/6drepnet\n","from sixdrepnet import SixDRepNet\n","\n","# Import models\n","from torchvision import models\n","from vit_pytorch.twins_svt import TwinsSVT # MEMORIA NON SUFFICIENTE RIPROVARLO\n","#from vit_pytorch.vit import ViT\n","from vit_pytorch.ats_vit import ViT\n","from vit_pytorch import SimpleViT\n","from vit_pytorch.crossformer import CrossFormer # MEMORIA NON SUFFICIENTE RIPROVARLO\n","from vit_pytorch.cross_vit import CrossViT"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["root_project = '/home/anto/University/Driving-Visual-Attention/'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We have  access to a GPU\n","0\n","<torch.cuda.device object at 0x7fdb4817eeb0>\n","1\n","NVIDIA GeForce RTX 3060 Laptop GPU\n","cuda\n"]}],"source":["print(f\"We have {'' if torch.cuda.is_available() else 'not'} access to a GPU\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(torch.cuda.current_device())\n","    print(torch.cuda.device(0))\n","    print(torch.cuda.device_count())\n","    print(torch.cuda.get_device_name(0))\n","print(device)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["##### Initialize pre-trained models for feature extraction"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Initialize face detector and facial landmarks predictor\n","face_detector = dlib.get_frontal_face_detector()\n","predictor = dlib.shape_predictor(\"/home/anto/University/Driving-Visual-Attention/data/data\")\n","headpose_extractor = SixDRepNet()"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loader and Visualization"]},{"cell_type":"markdown","metadata":{},"source":["##### Files where to write the paths and labels"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["percentage = 1\n","save_train_file = root_project + 'save/save_train' + str(percentage)\n","save_val_file = root_project + 'save/save_val' + str(percentage)\n","save_test_file = root_project + 'save/save_test' + str(percentage)"]},{"cell_type":"markdown","metadata":{},"source":["##### Train Loader"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","Loading data\n","Written data to file: /home/anto/University/Driving-Visual-Attention/save/save_train1\n"]}],"source":["train_dataset_classloader = DataLoaderVisualizer(root_project,save_train_file,percentage,'train')"]},{"cell_type":"markdown","metadata":{},"source":["##### Validtion Loader"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","Loading data\n","Written data to file: /home/anto/University/Driving-Visual-Attention/save/save_val1\n"]}],"source":["val_dataset_classloader = DataLoaderVisualizer(root_project,save_val_file,percentage,'val')"]},{"cell_type":"markdown","metadata":{},"source":["##### Test Loader"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","\n","Loading data\n","Written data to file: /home/anto/University/Driving-Visual-Attention/save/save_test1\n"]}],"source":["test_dataset_classloader = DataLoaderVisualizer(root_project,save_test_file,percentage,'test')"]},{"cell_type":"markdown","metadata":{},"source":["##### Visualization"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["#train_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["#val_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["#test_dataset_classloader.visualize_dataset()"]},{"cell_type":"markdown","metadata":{},"source":["### Pytorch Dataset "]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Crop and convert to tensor\n","crop_params = (125, 75, 768, 768)\n","# Choose size of the eyes\n","dim = (64,64)\n","# mean and std of images, calculated in advance\n","mean = (0.4570, 0.4422, 0.3900)\n","std = (0.2376, 0.2295, 0.2261)\n","\n","my_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    #CropTransform(crop_params),\n","    transforms.Resize(dim, antialias=True),\n","    transforms.Normalize(mean=mean, std=mean, inplace=True)\n","])"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataset len is 1054\n"]}],"source":["train_dataset = DGAZEDataset(headpose_extractor, predictor, face_detector,'train','save/save_train'+str(percentage),my_transforms)\n","print(f'Train dataset len is {len(train_dataset)}')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Val dataset len is 127\n"]}],"source":["val_dataset = DGAZEDataset(headpose_extractor, predictor, face_detector,'val','save/save_val'+str(percentage),my_transforms)\n","print(f'Val dataset len is {len(val_dataset)}')"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Test dataset len is 126\n"]}],"source":["test_dataset = DGAZEDataset(headpose_extractor, predictor, face_detector,'test','save/save_test'+str(percentage),my_transforms)\n","print(f'Test dataset len is {len(test_dataset)}')"]},{"cell_type":"markdown","metadata":{},"source":["### Vision Transformer Model"]},{"cell_type":"markdown","metadata":{},"source":["##### Hyerparameters"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["EPOCHS = 10\n","BATCH_SIZE = 16\n","'''\n","IMAGE_SIZE = 768\n","HEADS = 8\n","DROPOUT = 0.1\n","PATCH_SIZE = 32\n","DIM = 1024\n","MLP_DIM= 1024\n","DEPTH = 4\n","'''\n","THRESHOLD = 180\n","pre_trained = False"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"data":{"text/plain":["GazeCNN(\n","  (eye_feature_extractor): EyeFeatureExtractor(\n","    (conv1): Conv2d(3, 32, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n","    (relu): PReLU(num_parameters=1)\n","    (block): ConvolutionBlock(\n","      (conv_block): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (batch_norm_block): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (prelu_block): PReLU(num_parameters=1)\n","    )\n","    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (dropout): Dropout(p=0.25, inplace=False)\n","    (conv2): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (flatten): Flatten(start_dim=1, end_dim=-1)\n","  )\n","  (fc_additional): Sequential(\n","    (0): Linear(in_features=7, out_features=256, bias=True)\n","    (1): PReLU(num_parameters=1)\n","    (2): Linear(in_features=256, out_features=256, bias=True)\n","    (3): PReLU(num_parameters=1)\n","  )\n","  (fc_merge): Sequential(\n","    (0): Linear(in_features=4352, out_features=256, bias=True)\n","    (1): PReLU(num_parameters=1)\n","  )\n","  (fc_output): Linear(in_features=256, out_features=2, bias=True)\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","model = ViT(\n","    image_size = IMAGE_SIZE,\n","    patch_size = PATCH_SIZE,\n","    num_classes = 2,\n","    channels= 3,\n","    pool='mean',\n","    dim = DIM,\n","    depth = DEPTH,\n","    heads = HEADS,\n","    mlp_dim = MLP_DIM,\n","    dropout = DROPOUT,\n","    emb_dropout = 0.1\n",")\n","\n","model = SimpleViT(\n","    image_size = IMAGE_SIZE,\n","    patch_size = PATCH_SIZE,\n","    num_classes = 2,\n","    dim = 1024,\n","    depth = 6,\n","    heads = 16,\n","    mlp_dim = 2048\n",")\n","\n","model = CrossViT(\n","    image_size = IMAGE_SIZE,\n","    num_classes = 2,\n","    depth = 2,               # number of multi-scale encoding blocks\n","    sm_dim = 192,            # high res dimension\n","    sm_patch_size = 16,      # high res patch size (should be smaller than lg_patch_size)\n","    sm_enc_depth = 1,        # high res depth\n","    sm_enc_heads = 4,        # high res heads\n","    sm_enc_mlp_dim = 1024,   # high res feedforward dimension\n","    lg_dim = 384,            # low res dimension\n","    lg_patch_size = 64,      # low res patch size\n","    lg_enc_depth = 3,        # low res depth\n","    lg_enc_heads = 8,        # low res heads\n","    lg_enc_mlp_dim = 1024,   # low res feedforward dimensions\n","    cross_attn_depth = 1,    # cross attention rounds\n","    cross_attn_heads = 4,    # cross attention heads\n","    dropout = 0.1,\n","    emb_dropout = 0.1\n",")\n","\n","model = ViT(\n","    image_size = IMAGE_SIZE,\n","    patch_size = PATCH_SIZE,\n","    num_classes = 2,\n","    dim = 1024,\n","    depth = 6,\n","    max_tokens_per_depth = (256, 128, 64, 32, 16, 8), # a tuple that denotes the maximum number of tokens that any given layer should have. if the layer has greater than this amount, it will undergo adaptive token sampling\n","    heads = 8,\n","    mlp_dim = 2048,\n","    dropout = 0.1,\n","    emb_dropout = 0.1\n",")\n","\n","class GazeHeadResNet(nn.Module):\n","    def __init__(self):\n","        super(GazeHeadResNet, self).__init__()\n","        self.resnet34 = models.resnet34(pretrained=True)\n","        #self.resnet34.avgpool = nn.MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n","        self.resnet34.fc = nn.Linear(in_features=512, out_features=4, bias=True)\n","\n","    def forward(self, X):\n","        h = self.resnet34(X)\n","        gaze_hat = h[:, :2]\n","        head_hat = h[:, 2:]\n","        return gaze_hat\n","    \n","model = GazeHeadResNet()\n","'''\n","model = GazeCNN()\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["##### Criterion and Optimizer"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["criterion = nn.L1Loss()\n","optimizer = optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.95))\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=50000, gamma=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["##### Dataloader"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Training "]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["if pre_trained:\n","    ckpt_path = ''\n","    checkpoint = torch.load(ckpt_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mscardino-2020613\u001b[0m (\u001b[33mvesuvio-erutta\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/plain":["True"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["wandb.login()"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"6QZlMOAH0znJ"},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.16.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/anto/University/Driving-Visual-Attention/wandb/run-20240103_225143-oudcd6p8</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/vesuvio-erutta/GazeViT/runs/oudcd6p8' target=\"_blank\">Modello Nostro lr = 0.01 scheduler</a></strong> to <a href='https://wandb.ai/vesuvio-erutta/GazeViT' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/vesuvio-erutta/GazeViT' target=\"_blank\">https://wandb.ai/vesuvio-erutta/GazeViT</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/vesuvio-erutta/GazeViT/runs/oudcd6p8' target=\"_blank\">https://wandb.ai/vesuvio-erutta/GazeViT/runs/oudcd6p8</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/vesuvio-erutta/GazeViT/runs/oudcd6p8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7fda29546c10>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["wandb.init(project=\"GazeViT\", name=\"Modello Nostro lr = 0.01 scheduler\")"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 0:  66%|██████▌   | 43/65 [07:33<03:54, 10.65s/batch, batch loss=318.57]"]}],"source":["if pre_trained:\n","    start_epoch = checkpoint['epoch']\n","    EPOCHS = start_epoch + EPOCHS\n","else:\n","    start_epoch = 0\n","    EPOCHS = EPOCHS\n","\n","for epoch in range(start_epoch, EPOCHS):\n","    # Training\n","    train_loss = train_epoch(model, train_loader, criterion, scheduler, optimizer, device, epoch)\n","    wandb.log({\"epoch\": epoch + 1,\"train_loss\": train_loss})\n","\n","    # Validation\n","    val_loss, val_accuracy = validate(model, val_loader, THRESHOLD, criterion, device, epoch)\n","    wandb.log({\"epoch\": epoch + 1,\"val_loss\": val_loss})\n","    wandb.log({\"epoch\": epoch + 1,\"val_accuracy\": val_accuracy})\n","\n","    log_image(val_loader, model, device)\n","\n","# Finish the WandB run\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_dict = {\n","    'epoch': epoch,\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","}\n","torch.save(save_dict, root_project + 'save/AdaptiveToken_weights_epochs' + str(EPOCHS)+ '.pth')"]},{"cell_type":"markdown","metadata":{},"source":["### Test"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMTeTAy7+Meohhx8z6EH3FB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
