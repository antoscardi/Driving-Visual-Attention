{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Requirements"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from dataset import*\n","from utility import*\n","from training import*\n","from baseline import*\n","from torch.optim import lr_scheduler \n","\n","# Head pose DL model from https://github.com/thohemp/6drepnet\n","from sixdrepnet import SixDRepNet\n","\n","# Import models\n","from torchvision import models\n","from vit_pytorch.twins_svt import TwinsSVT # MEMORIA NON SUFFICIENTE RIPROVARLO\n","#from vit_pytorch.vit import ViT\n","from vit_pytorch.ats_vit import ViT\n","from vit_pytorch import SimpleViT\n","from vit_pytorch.crossformer import CrossFormer # MEMORIA NON SUFFICIENTE RIPROVARLO\n","from vit_pytorch.cross_vit import CrossViT"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["root_project = '/home/anto/University/Driving-Visual-Attention/'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["We have  access to a GPU\n","0\n","<torch.cuda.device object at 0x7f76c41b0160>\n","1\n","NVIDIA GeForce RTX 3060 Laptop GPU\n","cuda\n"]}],"source":["print(f\"We have {'' if torch.cuda.is_available() else 'not'} access to a GPU\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    print(torch.cuda.current_device())\n","    print(torch.cuda.device(0))\n","    print(torch.cuda.device_count())\n","    print(torch.cuda.get_device_name(0))\n","print(device)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["seed_everything(42)"]},{"cell_type":"markdown","metadata":{},"source":["##### Initialize pre-trained models for feature extraction"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Initialize face detector and facial landmarks predictor\n","predictor = dlib.shape_predictor(\"/home/anto/University/Driving-Visual-Attention/data/shape_predictor_68_face_landmarks.dat\")\n","headpose_extractor = SixDRepNet()\n","face_detector = dlib.get_frontal_face_detector()"]},{"cell_type":"markdown","metadata":{},"source":["### Data Loader and Visualization"]},{"cell_type":"markdown","metadata":{},"source":["##### Files where to write the paths and labels"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["percentage = 100\n","save_train_file = root_project + 'save/save_train' + str(percentage)\n","save_val_file = root_project + 'save/save_val' + str(percentage)\n","save_test_file = root_project + 'save/save_test' + str(percentage)"]},{"cell_type":"markdown","metadata":{},"source":["##### Train Loader"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Building path structure\n","Loading data\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▎        | 2/16 [34:14<4:00:01, 1028.68s/it]"]},{"name":"stdout","output_type":"stream","text":["In driver22 I skipped 3 images\n"]},{"name":"stderr","output_type":"stream","text":[" 25%|██▌       | 4/16 [1:04:53<3:14:23, 971.98s/it]"]},{"name":"stdout","output_type":"stream","text":["In driver3 I skipped 10 images\n"]},{"name":"stderr","output_type":"stream","text":[" 44%|████▍     | 7/16 [1:54:57<2:28:55, 992.86s/it]"]},{"name":"stdout","output_type":"stream","text":["In driver5 I skipped 613 images\n"]},{"name":"stderr","output_type":"stream","text":[" 50%|█████     | 8/16 [2:12:37<2:15:13, 1014.21s/it]"]},{"name":"stdout","output_type":"stream","text":["In driver8 I skipped 8 images\n"]},{"name":"stderr","output_type":"stream","text":[" 62%|██████▎   | 10/16 [2:51:17<1:49:02, 1090.50s/it]"]},{"name":"stdout","output_type":"stream","text":["In driver18 I skipped 22 images\n"]},{"name":"stderr","output_type":"stream","text":[" 81%|████████▏ | 13/16 [3:39:30<51:16, 1025.53s/it]  "]}],"source":["train_dataset_classloader = DataLoaderVisualizer(root_project, save_train_file, percentage, predictor, face_detector, headpose_extractor, 'train')"]},{"cell_type":"markdown","metadata":{},"source":["##### Validtion Loader"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_dataset_classloader = DataLoaderVisualizer(root_project,save_val_file,percentage,predictor, face_detector, headpose_extractor,'val')"]},{"cell_type":"markdown","metadata":{},"source":["##### Test Loader"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#test_dataset_classloader = DataLoaderVisualizer(root_project,save_test_file,percentage,predictor, face_detector, headpose_extractor,'test')"]},{"cell_type":"markdown","metadata":{},"source":["##### Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#train_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#val_dataset_classloader.visualize_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#test_dataset_classloader.visualize_dataset()"]},{"cell_type":"markdown","metadata":{},"source":["### Pytorch Dataset "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Choose size of the eyes\n","dim = (64,128)\n","# mean and std of images, calculated in advance\n","mean = (0.4570, 0.4422, 0.3900)\n","std = (0.2376, 0.2295, 0.2261)\n","\n","my_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Resize(dim, antialias=True),\n","    #transforms.Normalize(mean=mean, std=mean, inplace=True)\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_dataset = DGAZEDataset('train','save/save_train'+str(percentage),my_transforms)\n","print(f'Train dataset len is {len(train_dataset)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print an example of the dataset for correct visualization\n","img_np = train_dataset[30][0].permute(1, 2, 0).numpy()\n","plt.imshow(img_np)\n","plt.axis('off')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["val_dataset = DGAZEDataset('val','save/save_val'+str(percentage),my_transforms)\n","print(f'Val dataset len is {len(val_dataset)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#test_dataset = DGAZEDataset('test','save/save_test'+str(percentage),my_transforms)\n","#print(f'Test dataset len is {len(test_dataset)}')"]},{"cell_type":"markdown","metadata":{},"source":["### Vision Transformer Model"]},{"cell_type":"markdown","metadata":{},"source":["##### Hyerparameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["EPOCHS = 20\n","BATCH_SIZE = 32\n","THRESHOLD = 200\n","pre_trained = False"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = GazeCNN()\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["##### Criterion and Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["criterion = nn.L1Loss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.95))\n","scheduler = lr_scheduler.StepLR(optimizer, step_size=50000, gamma=0.1)"]},{"cell_type":"markdown","metadata":{},"source":["##### Dataloader"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["### Training "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if pre_trained:\n","    ckpt_path = ''\n","    checkpoint = torch.load(ckpt_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QZlMOAH0znJ"},"outputs":[],"source":["wandb.init(project=\"GazeViT\", name=f\"Modello Nostro lr = 0.01 scheduler RGB 64 batch_size images no normalization {percentage} percent\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if pre_trained:\n","    start_epoch = checkpoint['epoch']\n","    EPOCHS = start_epoch + EPOCHS\n","else:\n","    start_epoch = 0\n","    EPOCHS = EPOCHS\n","\n","for epoch in range(start_epoch, EPOCHS):\n","    # Training\n","    train_loss = train_epoch(model, train_loader, criterion, scheduler, optimizer, device, epoch)\n","    wandb.log({\"epoch\": epoch + 1,\"train_loss\": train_loss})\n","\n","    # Validation\n","    val_loss, val_accuracy = validate(model, val_loader, THRESHOLD, criterion, device, epoch)\n","    wandb.log({\"epoch\": epoch + 1,\"val_loss\": val_loss})\n","    wandb.log({\"epoch\": epoch + 1,\"val_accuracy\": val_accuracy})\n","\n","    log_image(val_loader, model, device)\n","\n","# Finish the WandB run\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_dict = {\n","    'epoch': epoch,\n","    'model_state_dict': model.state_dict(),\n","    'optimizer_state_dict': optimizer.state_dict(),\n","}\n","torch.save(save_dict, root_project + 'save/our_baseline_epochs' + str(EPOCHS)+ '.pth')"]},{"cell_type":"markdown","metadata":{},"source":["### Test"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMTeTAy7+Meohhx8z6EH3FB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
